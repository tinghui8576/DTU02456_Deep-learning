Step 500     training accuracy: 0.0993125
             test accuracy: 0.0999
Step 1000    training accuracy: 0.09611525229357798
             test accuracy: 0.0999
Step 1500    training accuracy: 0.10159375
             test accuracy: 0.0999
Step 2000    training accuracy: 0.0989822247706422
             test accuracy: 0.0999
Step 2500    training accuracy: 0.09912743506493507
             test accuracy: 0.0999
Step 3000    training accuracy: 0.10084375
             test accuracy: 0.0999
Step 3500    training accuracy: 0.10089045698924731
             test accuracy: 0.0999
Step 4000    training accuracy: 0.09513888888888888
             test accuracy: 0.0999
Step 4500    training accuracy: 0.09978125
             test accuracy: 0.0999
Model(
  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 25, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (fc1): Linear(in_features=625, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=10, bias=True)
)
Model(
  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 25, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=625, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=10, bias=True)
)
Model(
  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 25, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=625, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=10, bias=True)
)
Output shape: torch.Size([2, 25, 12, 12])
Output logits:
[[[[0.         0.         0.65766275 ... 0.67637247 0.7186712
    0.36555907]
   [0.42440388 0.         0.40735674 ... 0.8783588  0.3295544
    0.608362  ]
   [0.12186459 0.18895678 0.         ... 0.43407366 0.49665505
    0.34169665]
   ...
   [0.5027488  0.45438695 0.6405746  ... 0.53690505 0.49253
    0.        ]
   [0.156387   0.12600334 0.311388   ... 0.52386105 1.019663
    0.04577177]
   [0.38979918 0.1260378  0.02370132 ... 0.6978439  0.75580895
    0.2721369 ]]

  [[1.1013184  0.38694876 0.01882372 ... 0.16796692 0.17897168
    0.1290628 ]
   [0.11154025 0.42934948 0.629163   ... 0.         0.3408517
    0.37850133]
   [0.         0.34868985 0.08861635 ... 0.42463377 0.42851472
    0.480358  ]
   ...
   [1.0579442  0.17187989 0.23246486 ... 0.29738817 0.055657
    0.06885058]
   [0.39956605 0.44604862 0.5512221  ... 0.         0.07183554
    0.04754925]
   [0.40998247 0.50613195 0.41189274 ... 0.23421967 0.20172766
    0.5174251 ]]

  [[0.15406358 0.         0.26240435 ... 0.25837928 0.5555347
    0.10262594]
   [0.13587663 0.29922456 0.02282659 ... 0.31074852 0.16829312
    0.56614983]
   [0.43094662 0.50134426 0.         ... 0.19734003 0.39232603
    0.        ]
   ...
   [0.         0.17483945 0.         ... 0.5642756  0.8243727
    0.54418176]
   [0.         0.29417562 0.09878216 ... 1.1131715  0.57767314
    0.22839548]
   [0.         0.23984987 0.         ... 0.18575284 0.17768227
    0.32749876]]

  ...

  [[1.0881895  0.47363883 0.48918298 ... 0.35187638 0.40973508
    0.19705765]
   [0.11474509 0.26451033 0.8975982  ... 0.43463    0.35653996
    0.824384  ]
   [0.3970952  1.0034345  0.32609203 ... 0.42447588 0.
    1.0335021 ]
   ...
   [0.48346868 0.18920797 0.481372   ... 0.         0.48655364
    0.44493687]
   [0.22633249 0.63365626 0.05841714 ... 0.28157383 0.66288537
    0.        ]
   [0.5169539  0.5441983  0.88310456 ... 0.00577865 0.5152488
    0.        ]]

  [[0.15839002 0.09889115 0.14130297 ... 0.1165107  0.
    0.29869884]
   [0.33468837 0.4132306  0.5235566  ... 0.5361023  0.27363616
    0.1070189 ]
   [0.20093906 0.47362196 0.         ... 0.         0.37769225
    0.35420933]
   ...
   [0.4512635  0.         0.1258158  ... 0.10019969 0.1319131
    0.19779202]
   [0.02911603 0.54134244 0.18518843 ... 0.3209513  0.00771146
    0.28822374]
   [0.         0.39603478 0.24318476 ... 0.21880552 0.25971547
    0.19353215]]

  [[0.5493036  0.3535869  0.5235899  ... 0.5114151  0.1978908
    0.5985371 ]
   [0.17020893 0.36944908 0.         ... 0.23005207 0.3775852
    0.586217  ]
   [0.         0.         0.30046147 ... 0.0845712  0.04348013
    0.3206369 ]
   ...
   [0.8347744  0.46274668 0.22289829 ... 0.04218107 0.
    0.569131  ]
   [0.45230776 0.19439647 0.         ... 0.25642538 0.
    0.        ]
   [0.93219465 0.2112553  0.08957735 ... 0.12196764 0.5213882
    0.13411461]]]


 [[[0.4857017  0.28617382 0.03956119 ... 0.29779267 0.9307982
    0.27524757]
   [0.5674484  0.2833819  0.5869608  ... 0.6097543  0.3519371
    0.60975134]
   [0.23254779 0.         0.30740872 ... 0.48261213 0.11474875
    1.005629  ]
   ...
   [0.68507135 0.46599162 0.63707095 ... 0.41031477 0.28257412
    0.        ]
   [0.19594973 0.8045721  0.5140761  ... 0.82903945 0.40733558
    0.20121203]
   [0.4407377  0.01661859 0.32348052 ... 0.5852978  0.24602263
    0.7783901 ]]

  [[0.64853394 0.15519926 0.3167419  ... 0.35551918 0.62776893
    0.76883566]
   [0.3450434  0.32881922 0.06059074 ... 0.09464907 0.9912189
    0.04736269]
   [0.2632492  0.17490497 0.57804114 ... 0.22162543 0.3064991
    0.19475473]
   ...
   [0.1991618  0.43643206 0.53652036 ... 0.         0.12027382
    0.        ]
   [0.         0.21712494 0.3678905  ... 0.01643227 0.54678655
    0.        ]
   [0.17695713 0.15122244 0.44918418 ... 0.         0.
    0.21901591]]

  [[0.2295184  0.18117456 0.         ... 0.8861369  0.19149235
    0.        ]
   [0.41476288 0.53064126 0.16790695 ... 0.         0.16446248
    0.        ]
   [0.18941496 0.539844   0.3779345  ... 0.         0.
    0.6114749 ]
   ...
   [0.5924055  0.35112357 0.23744962 ... 0.         0.1710538
    0.60274535]
   [0.1955083  0.1198374  0.22736193 ... 0.71661735 0.3649291
    0.05657304]
   [0.         0.23755898 0.37824824 ... 0.3286729  0.30254745
    0.5346275 ]]

  ...

  [[0.2974845  0.35186955 0.66858405 ... 0.41536936 1.1051672
    0.29580623]
   [0.03732498 0.28290048 0.5414233  ... 0.40700367 1.0084648
    0.38941842]
   [0.02174703 0.2381536  0.29860696 ... 0.5731123  0.20382524
    0.33201578]
   ...
   [0.29965296 0.05502455 0.58046573 ... 0.         0.48659405
    0.25204125]
   [0.208557   0.7550968  0.23589125 ... 0.47187516 0.5393807
    0.        ]
   [0.24786016 0.4117517  0.78284675 ... 0.31296146 0.
    0.09161642]]

  [[0.3533156  0.53006876 0.30367544 ... 0.         0.
    0.36929342]
   [0.29779556 0.22885212 0.11754221 ... 0.29814053 0.17201447
    0.4351938 ]
   [0.24163446 0.01550067 0.06759766 ... 0.2284034  0.70181733
    0.23985621]
   ...
   [0.38446963 0.         0.13577496 ... 0.         0.6033466
    0.32370204]
   [0.33170953 0.39263904 0.1685027  ... 0.05509863 0.4435743
    0.11051151]
   [0.34831545 0.11065685 0.35801136 ... 0.6207234  0.1263249
    0.2910492 ]]

  [[0.2352912  0.15660308 0.03943287 ... 0.37545568 0.08072162
    0.28476772]
   [0.         0.2704362  0.42345852 ... 0.2415578  0.42097247
    0.6192972 ]
   [0.         0.         0.09950594 ... 0.11000703 0.9652952
    0.        ]
   ...
   [0.17848639 0.28515217 0.4093582  ... 0.11768395 0.
    0.51220495]
   [0.26117322 0.08569259 0.23404562 ... 0.16665113 0.12206504
    0.        ]
   [0.11036305 0.61917675 0.66864747 ... 0.         0.17162998
    0.1824063 ]]]]
Output probabilities:
[[[[0.02654688 0.02910152 0.0546106  ... 0.05393929 0.05753098
    0.04012248]
   [0.04547018 0.02779401 0.04305437 ... 0.06733771 0.04169618
    0.04761434]
   [0.03243954 0.03267575 0.02910822 ... 0.04340302 0.04930265
    0.04043831]
   ...
   [0.04476182 0.04630093 0.05671129 ... 0.04980724 0.044873
    0.02960983]
   [0.03452802 0.03240699 0.04140097 ... 0.04635524 0.07497484
    0.03092577]
   [0.03908206 0.02960902 0.03086074 ... 0.05523651 0.05882388
    0.03686943]]

  [[0.07985643 0.04285144 0.02882921 ... 0.032442   0.03353615
    0.03167224]
   [0.0332546  0.04269876 0.05374604 ... 0.02797641 0.0421699
    0.03783646]
   [0.0287177  0.03833511 0.03180543 ... 0.04299523 0.04605505
    0.04645289]
   ...
   [0.07798769 0.03490587 0.03770767 ... 0.0391987  0.02899036
    0.03172031]
   [0.04403347 0.04463058 0.05262221 ... 0.02745294 0.02905891
    0.03098079]
   [0.03987888 0.04330089 0.04549836 ... 0.03474376 0.03380019
    0.04711875]]

  [[0.03096866 0.02910152 0.03678057 ... 0.03551184 0.04887114
    0.0308459 ]
   [0.03407383 0.0374889  0.02931014 ... 0.0381723  0.03548635
    0.04564626]
   [0.04418831 0.04465743 0.02910822 ... 0.03425373 0.04441818
    0.02873399]
   ...
   [0.02707489 0.03500933 0.02988626 ... 0.05118933 0.06253204
    0.0510236 ]
   [0.02952933 0.03834201 0.03347164 ... 0.08356666 0.04819055
    0.03712215]
   [0.02646609 0.03317813 0.03013789 ... 0.03309999 0.03299715
    0.03896815]]

  ...

  [[0.07881486 0.046732   0.04614314 ... 0.03899226 0.04224084
    0.03390069]
   [0.03336135 0.03620983 0.07029532 ... 0.04320653 0.04283669
    0.05909557]
   [0.04271751 0.07378172 0.04033069 ... 0.04298844 0.03000376
    0.08076816]
   ...
   [0.04390707 0.03551599 0.04836473 ... 0.02911505 0.04460562
    0.04620294]
   [0.03702956 0.05384056 0.03214746 ... 0.03638101 0.05247701
    0.02954216]
   [0.0443813  0.04498097 0.07288537 ... 0.02764815 0.04624659
    0.02808528]]

  [[0.03110293 0.03212651 0.03258553 ... 0.03081487 0.02804057
    0.0375276 ]
   [0.04156844 0.04201602 0.04835954 ... 0.04782096 0.03942859
    0.02884079]
   [0.03510883 0.04343643 0.02910822 ... 0.02811928 0.04377291
    0.04094747]
   ...
   [0.04251556 0.02939355 0.03389321 ... 0.03218354 0.03128752
    0.03608577]
   [0.03040175 0.04909284 0.03649243 ... 0.03784219 0.02725402
    0.03941089]
   [0.02646609 0.03878664 0.03843499 ... 0.03421232 0.03581814
    0.03408229]]

  [[0.04598042 0.04144542 0.04775841 ... 0.04573674 0.03417667
    0.05064877]
   [0.03526397 0.04021618 0.02864867 ... 0.03521294 0.04374775
    0.04657151]
   [0.0287177  0.02704971 0.03931012 ... 0.03060081 0.03133711
    0.03959559]
   ...
   [0.06238851 0.04668962 0.03734866 ... 0.03036943 0.02742092
    0.05231261]
   [0.04641821 0.03470095 0.0303233  ... 0.0354775  0.02704466
    0.02954216]
   [0.06722593 0.03224286 0.03296218 ... 0.03105463 0.04653139
    0.0321162 ]]]


 [[[0.04637728 0.04047421 0.03166508 ... 0.03689108 0.06567067
    0.03639782]
   [0.05199466 0.03917693 0.05489332 ... 0.05035267 0.03793596
    0.04910292]
   [0.03816257 0.0296823  0.0382656  ... 0.04577219 0.0299265
    0.07507328]
   ...
   [0.05624574 0.0451522  0.04768084 ... 0.04262836 0.03725967
    0.02697674]
   [0.03549465 0.06179986 0.04839758 ... 0.0631723  0.04110209
    0.03847074]
   [0.04618762 0.02952652 0.03777513 ... 0.05305635 0.03703938
    0.05713441]]

  [[0.05457861 0.0355056  0.04177906 ... 0.03908334 0.04850288
    0.05962631]
   [0.04162646 0.04099809 0.03242798 ... 0.03008258 0.07189318
    0.02798111]
   [0.03935238 0.03535557 0.05015828 ... 0.03525792 0.036252
    0.03336777]
   ...
   [0.03459886 0.04383706 0.04311966 ... 0.02828141 0.03167764
    0.02697674]
   [0.02917851 0.03434492 0.04181538 ... 0.02802953 0.04725271
    0.03145902]
   [0.03547866 0.0337808  0.04283497 ... 0.0295492  0.02896126
    0.03265613]]

  [[0.03589602 0.03643995 0.03043683 ... 0.06644098 0.03135415
    0.02763993]
   [0.04463219 0.0501665  0.03610162 ... 0.02736589 0.03145082
    0.02668675]
   [0.0365515  0.05092708 0.04106175 ... 0.02824919 0.02668217
    0.05061818]
   ...
   [0.05126789 0.04025245 0.03197353 ... 0.02828141 0.03332777
    0.04928996]
   [0.03547899 0.03116097 0.03633334 ... 0.056455   0.03939554
    0.03329006]
   [0.02972458 0.03682692 0.0399017  ... 0.04104749 0.03919333
    0.04477473]]

  ...

  [[0.03842055 0.04322248 0.05939663 ... 0.0414939  0.07818059
    0.03715386]
   [0.03060053 0.03915808 0.05244968 ... 0.04111204 0.07314379
    0.0393929 ]
   [0.03090921 0.037664   0.03793027 ... 0.05010782 0.03271458
    0.03827709]
   ...
   [0.03825644 0.0299363  0.04505682 ... 0.02828141 0.04569238
    0.0347096 ]
   [0.03594498 0.0588167  0.03664456 ... 0.04419898 0.04690406
    0.03145902]
   [0.03808553 0.04383453 0.0598007  ... 0.04040761 0.02896126
    0.02874986]]

  [[0.04062663 0.0516536  0.04123671 ... 0.02738998 0.02588993
    0.03998702]
   [0.03970544 0.03709783 0.0343284  ... 0.03687146 0.03168923
    0.04123803]
   [0.03851092 0.03014598 0.03010647 ... 0.03549771 0.05382903
    0.03490716]
   ...
   [0.04164281 0.02833357 0.02888243 ... 0.02828141 0.05135098
    0.0372882 ]
   [0.04065581 0.04093429 0.0342565  ... 0.02913456 0.0426189
    0.03513499]
   [0.04211019 0.03243788 0.03910232 ... 0.05496959 0.03286091
    0.03509525]]

  [[0.03610384 0.03555547 0.03166102 ... 0.03987034 0.02806648
    0.03674599]
   [0.02947943 0.03867303 0.04661346 ... 0.0348431  0.0406474
    0.04957389]
   [0.03024429 0.0296823  0.03108261 ... 0.03153417 0.07005571
    0.0274629 ]
   ...
   [0.03389087 0.03768265 0.03797078 ... 0.03181344 0.02808786
    0.04502329]
   [0.03788691 0.03011495 0.03657699 ... 0.0325728  0.030901
    0.03145902]
   [0.03319294 0.05393863 0.05334701 ... 0.0295492  0.03438392
    0.03148222]]]]
Model(
  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 25, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=625, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=10, bias=True)
)
Output shape: torch.Size([2, 25, 2, 2])
Output logits:
[[[[0.         0.        ]
   [0.         0.        ]]

  [[0.08543182 0.0809321 ]
   [0.05793626 0.0846325 ]]

  [[0.28431416 0.12979582]
   [0.09469332 0.19560754]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.1818481  0.19528277]
   [0.16773501 0.20923166]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.13637663 0.05583591]
   [0.06911308 0.1578523 ]]

  [[0.00560093 0.07173453]
   [0.00818676 0.03326706]]

  [[0.18168552 0.11480276]
   [0.12546767 0.05758562]]

  [[0.13354488 0.18085615]
   [0.1561341  0.11265554]]

  [[0.18144093 0.22519098]
   [0.16975524 0.22746462]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.2957477  0.21495588]
   [0.22497775 0.18652213]]

  [[0.20459087 0.12263864]
   [0.12214016 0.11207816]]

  [[0.07808378 0.04786728]
   [0.10880988 0.00994148]]

  [[0.         0.        ]
   [0.         0.02291302]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.10004898 0.07316245]
   [0.1632088  0.10750198]]

  [[0.13505396 0.13198549]
   [0.17641148 0.11317305]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.10363109 0.04922602]
   [0.07303848 0.08218719]]

  [[0.26339376 0.2738776 ]
   [0.20676227 0.26117602]]

  [[0.17515779 0.16854773]
   [0.10721083 0.1505633 ]]]


 [[[0.         0.        ]
   [0.         0.        ]]

  [[0.00390838 0.11196975]
   [0.02871205 0.0429699 ]]

  [[0.15311053 0.21578105]
   [0.10335051 0.19961095]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.11103708 0.13415681]
   [0.22655389 0.14104177]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.14016886 0.1797519 ]
   [0.13614416 0.11775073]]

  [[0.05559066 0.05189766]
   [0.02648853 0.04026109]]

  [[0.05710327 0.06123959]
   [0.07278063 0.07547382]]

  [[0.11415803 0.17254698]
   [0.08599166 0.14651419]]

  [[0.2271717  0.29196736]
   [0.3084949  0.2790308 ]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.23322158 0.2372234 ]
   [0.21451588 0.2174974 ]]

  [[0.13167088 0.11643218]
   [0.15229735 0.13023226]]

  [[0.097914   0.04334353]
   [0.02811361 0.08467189]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.12690614 0.09056854]
   [0.11740499 0.10929273]]

  [[0.04204527 0.0856713 ]
   [0.07427828 0.0915932 ]]

  [[0.02137879 0.        ]
   [0.         0.01018068]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.         0.        ]
   [0.         0.        ]]

  [[0.11576397 0.03143414]
   [0.1149753  0.01230913]]

  [[0.2718401  0.2326532 ]
   [0.21141617 0.19382983]]

  [[0.14679213 0.14127402]
   [0.22676916 0.1575789 ]]]]
Output probabilities:
[[[[0.03595259 0.03659223]
   [0.0367714  0.03661315]]

  [[0.03915911 0.03967685]
   [0.03896472 0.03984671]]

  [[0.04777561 0.04166376]
   [0.0404236  0.0445234 ]]

  [[0.03595259 0.03659223]
   [0.0367714  0.03661315]]

  [[0.04312269 0.04448351]
   [0.04348671 0.04513415]]

  [[0.03595259 0.03659223]
   [0.0367714  0.03661315]]

  [[0.04120575 0.0386935 ]
   [0.03940267 0.04287375]]

  [[0.03615452 0.03931359]
   [0.03707368 0.03785165]]

  [[0.04311568 0.04104375]
   [0.04168695 0.03878343]]

  [[0.04108923 0.04384637]
   [0.04298514 0.04097913]]

  [[0.04310513 0.04583403]
   [0.04357465 0.04596462]]

  [[0.03595259 0.03659223]
   [0.0367714  0.03661315]]

  [[0.04832499 0.0453673 ]
   [0.04604864 0.04412072]]

  [[0.04411466 0.04136663]
   [0.04154846 0.04095548]]

  [[0.03887242 0.0383864 ]
   [0.04099829 0.03697895]]

  [[0.03595259 0.03659223]
   [0.0367714  0.03746175]]

  [[0.03595259 0.03659223]
   [0.0367714  0.03661315]]

  [[0.03973571 0.03936977]
   [0.04329033 0.04076849]]

  [[0.04115129 0.04175509]
   [0.04386567 0.04100034]]

  [[0.03595259 0.03659223]
   [0.0367714  0.03661315]]

  [[0.03595259 0.03659223]
   [0.0367714  0.03661315]]

  [[0.03595259 0.03659223]
   [0.0367714  0.03661315]]

  [[0.0398783  0.03843859]
   [0.03955764 0.0397494 ]]

  [[0.04678651 0.04812075]
   [0.04521743 0.04754057]]

  [[0.04283515 0.04331   ]
   [0.04093278 0.04256238]]]


 [[[0.03672619 0.03648439]
   [0.03657991 0.03672502]]

  [[0.03687001 0.04080703]
   [0.03764542 0.03833749]]

  [[0.04280268 0.04527095]
   [0.04056273 0.04483859]]

  [[0.03672619 0.03648439]
   [0.03657991 0.03672502]]

  [[0.04103918 0.04172253]
   [0.04588109 0.04228786]]

  [[0.03672619 0.03648439]
   [0.03657991 0.03672502]]

  [[0.04225231 0.04366891]
   [0.04191498 0.04131432]]

  [[0.03882564 0.03842784]
   [0.03756181 0.03823378]]

  [[0.03888441 0.03878851]
   [0.0393415  0.03960408]]

  [[0.04116746 0.04335541]
   [0.03986469 0.04251991]]

  [[0.04609303 0.04885476]
   [0.04979896 0.0485449 ]]

  [[0.03672619 0.03648439]
   [0.03657991 0.03672502]]

  [[0.04637273 0.04625215]
   [0.04533209 0.04564781]]

  [[0.04189477 0.04098953]
   [0.04259754 0.04183321]]

  [[0.04050414 0.03810053]
   [0.0376229  0.03997004]]

  [[0.03672619 0.03648439]
   [0.03657991 0.03672502]]

  [[0.03672619 0.03648439]
   [0.03657991 0.03672502]]

  [[0.04169562 0.03994299]
   [0.04113685 0.04096635]]

  [[0.03830327 0.03974786]
   [0.03940046 0.04024765]]

  [[0.03751981 0.03648439]
   [0.03657991 0.03710081]]

  [[0.03672619 0.03648439]
   [0.03657991 0.03672502]]

  [[0.03672619 0.03648439]
   [0.03657991 0.03672502]]

  [[0.04123363 0.03764946]
   [0.04103702 0.03717986]]

  [[0.04819861 0.04604125]
   [0.04519179 0.04458012]]

  [[0.04253308 0.04202054]
   [0.04589097 0.04299299]]]]
Model(
  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 25, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=625, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=10, bias=True)
)
Model(
  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 25, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=625, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=10, bias=True)
)
Model(
  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 25, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=625, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=10, bias=True)
)
torch.Size([2, 100])
Model(
  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 25, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=625, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=10, bias=True)
)
torch.Size([2, 25, 2, 2])
torch.Size([2, 100])
Model(
  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 25, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=100, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=10, bias=True)
)
torch.Size([2, 25, 2, 2])
torch.Size([2, 100])
Output shape: torch.Size([2, 200])
Output logits:
[[0.         0.14642204 0.00515385 0.         0.00638922 0.
  0.         0.         0.0865531  0.04203833 0.00893829 0.03280199
  0.         0.         0.07052148 0.13863477 0.         0.01350922
  0.06462931 0.04406657 0.         0.02404659 0.11846638 0.
  0.01342169 0.         0.         0.10090162 0.         0.03558427
  0.         0.         0.         0.         0.         0.
  0.06739917 0.08263846 0.11385743 0.08422398 0.00684624 0.03009591
  0.02265969 0.         0.01929667 0.01507163 0.12407154 0.
  0.09548054 0.10875204 0.02967988 0.         0.         0.
  0.         0.         0.04815139 0.         0.06799514 0.
  0.03883929 0.         0.0504467  0.         0.0273601  0.08451847
  0.02502342 0.         0.         0.         0.02226858 0.02193935
  0.03191156 0.         0.01439358 0.04155583 0.         0.
  0.         0.         0.08751422 0.05864047 0.         0.
  0.07621714 0.         0.         0.09477032 0.         0.0693211
  0.00744353 0.         0.         0.01916869 0.         0.
  0.         0.         0.04351978 0.         0.         0.03508037
  0.         0.         0.07943144 0.         0.02391398 0.
  0.         0.         0.05131493 0.         0.         0.0034168
  0.         0.03896317 0.         0.15021689 0.07596071 0.08381855
  0.         0.07641486 0.04734834 0.07404839 0.         0.06466772
  0.         0.         0.         0.03206118 0.04834224 0.03885397
  0.         0.11515077 0.         0.         0.         0.04938382
  0.00911237 0.         0.06305496 0.07205747 0.         0.
  0.03827504 0.         0.         0.         0.07119066 0.04395563
  0.         0.02727302 0.         0.02188069 0.         0.
  0.         0.         0.         0.         0.03396773 0.
  0.         0.         0.12161863 0.0633213  0.         0.02472072
  0.         0.03461836 0.12539612 0.08497067 0.01521957 0.12300963
  0.03625708 0.02919223 0.06044655 0.00889529 0.09356716 0.01528795
  0.03413805 0.         0.0236899  0.0441825  0.10726824 0.
  0.         0.         0.         0.0522173  0.         0.13438194
  0.         0.         0.         0.02395859 0.         0.
  0.         0.        ]
 [0.         0.1281892  0.         0.         0.01413974 0.
  0.         0.         0.09655876 0.02635462 0.02323973 0.02957302
  0.         0.         0.07256296 0.13506614 0.         0.0106278
  0.06574921 0.03777295 0.         0.02240584 0.13084248 0.
  0.0101735  0.00392922 0.         0.10047147 0.         0.02836493
  0.         0.         0.         0.         0.         0.
  0.06917783 0.0739307  0.1083318  0.09804429 0.01163574 0.03180691
  0.02516469 0.         0.01878435 0.         0.14816651 0.
  0.10567584 0.11986197 0.01540755 0.00716642 0.         0.
  0.         0.         0.04834582 0.00041219 0.06854099 0.
  0.03476431 0.         0.02676667 0.         0.04550553 0.08291854
  0.02379532 0.         0.         0.         0.0356105  0.01801275
  0.0325377  0.00692885 0.02523117 0.03821982 0.         0.
  0.         0.         0.09281573 0.06064511 0.         0.
  0.07508613 0.         0.         0.10836307 0.0015382  0.05391775
  0.00447698 0.         0.         0.02208676 0.         0.
  0.         0.         0.05263041 0.         0.         0.02412442
  0.         0.         0.08624263 0.         0.03215436 0.
  0.         0.         0.06381371 0.         0.         0.00129597
  0.         0.03318432 0.         0.14640315 0.07475278 0.08812094
  0.         0.07577149 0.03493106 0.08050425 0.         0.05593521
  0.         0.         0.         0.04169082 0.05342666 0.05979754
  0.         0.10599104 0.         0.         0.         0.06427906
  0.0132407  0.         0.07697838 0.08136176 0.         0.
  0.04027569 0.         0.         0.         0.07403799 0.03951865
  0.         0.04399252 0.         0.00879748 0.         0.
  0.         0.         0.         0.         0.03968036 0.
  0.         0.         0.11050999 0.05893586 0.         0.02105665
  0.         0.0420114  0.10835453 0.08569898 0.         0.12004337
  0.04015919 0.04979691 0.06945591 0.01016227 0.0897342  0.0037617
  0.03446808 0.         0.02192101 0.04498944 0.11405447 0.
  0.         0.         0.01516314 0.05732361 0.         0.12893769
  0.         0.         0.         0.03733358 0.         0.00068368
  0.0073913  0.        ]]
Output probabilities:
[[0.00486306 0.00562989 0.00488818 0.00486306 0.00489423 0.00486306
  0.00486306 0.00486306 0.00530272 0.00507185 0.00490672 0.00502522
  0.00486306 0.00486306 0.00521839 0.00558621 0.00486306 0.0049292
  0.00518773 0.00508215 0.00486306 0.00498141 0.00547468 0.00486306
  0.00492877 0.00486306 0.00486306 0.00537936 0.00486306 0.00503922
  0.00486306 0.00486306 0.00486306 0.00486306 0.00486306 0.00486306
  0.00520212 0.005282   0.0054495  0.00529039 0.00489646 0.00501164
  0.00497451 0.00486306 0.00495781 0.00493691 0.00550545 0.00486306
  0.00535027 0.00542175 0.00500955 0.00486306 0.00486306 0.00486306
  0.00486306 0.00486306 0.00510295 0.00486306 0.00520522 0.00486306
  0.00505565 0.00486306 0.00511468 0.00486306 0.00499795 0.00529194
  0.00498628 0.00486306 0.00486306 0.00486306 0.00497256 0.00497093
  0.00502075 0.00486306 0.00493356 0.0050694  0.00486306 0.00486306
  0.00486306 0.00486306 0.00530782 0.00515676 0.00486306 0.00486306
  0.0052482  0.00486306 0.00486306 0.00534647 0.00486306 0.00521213
  0.00489939 0.00486306 0.00486306 0.00495717 0.00486306 0.00486306
  0.00486306 0.00486306 0.00507937 0.00486306 0.00486306 0.00503668
  0.00486306 0.00486306 0.00526509 0.00486306 0.00498075 0.00486306
  0.00486306 0.00486306 0.00511912 0.00486306 0.00486306 0.0048797
  0.00486306 0.00505628 0.00486306 0.00565129 0.00524685 0.00528824
  0.00486306 0.00524923 0.00509885 0.00523683 0.00486306 0.00518793
  0.00486306 0.00486306 0.00486306 0.0050215  0.00510392 0.00505572
  0.00486306 0.00545656 0.00486306 0.00486306 0.00486306 0.00510924
  0.00490757 0.00486306 0.00517957 0.00522641 0.00486306 0.00486306
  0.0050528  0.00486306 0.00486306 0.00486306 0.00522188 0.00508158
  0.00486306 0.00499751 0.00486306 0.00497064 0.00486306 0.00486306
  0.00486306 0.00486306 0.00486306 0.00486306 0.00503108 0.00486306
  0.00486306 0.00486306 0.00549196 0.00518095 0.00486306 0.00498477
  0.00486306 0.00503436 0.00551275 0.00529434 0.00493764 0.00549961
  0.00504261 0.00500711 0.00516608 0.00490651 0.00534005 0.00493797
  0.00503194 0.00486306 0.00497964 0.00508274 0.00541371 0.00486306
  0.00486306 0.00486306 0.00486306 0.00512374 0.00486306 0.00556251
  0.00486306 0.00486306 0.00486306 0.00498098 0.00486306 0.00486306
  0.00486306 0.00486306]
 [0.00485975 0.00552441 0.00485975 0.00485975 0.00492896 0.00485975
  0.00485975 0.00485975 0.00535241 0.00498953 0.00497401 0.00500562
  0.00485975 0.00485975 0.0052255  0.00556253 0.00485975 0.00491168
  0.00519001 0.00504683 0.00485975 0.00496987 0.00553909 0.00485975
  0.00490944 0.00487888 0.00485975 0.00537339 0.00485975 0.00499957
  0.00485975 0.00485975 0.00485975 0.00485975 0.00485975 0.00485975
  0.00520784 0.00523265 0.00541579 0.00536036 0.00491663 0.00501681
  0.0049836  0.00485975 0.0049519  0.00485975 0.00563588 0.00485975
  0.00540143 0.0054786  0.00493521 0.0048947  0.00485975 0.00485975
  0.00485975 0.00485975 0.00510047 0.00486176 0.00520452 0.00485975
  0.00503167 0.00485975 0.00499159 0.00485975 0.00508601 0.00527989
  0.00497678 0.00485975 0.00485975 0.00485975 0.00503593 0.00494808
  0.00502048 0.00489354 0.00498393 0.00504909 0.00485975 0.00485975
  0.00485975 0.00485975 0.00533241 0.00516359 0.00485975 0.00485975
  0.0052387  0.00485975 0.00485975 0.00541596 0.00486723 0.00512897
  0.00488156 0.00485975 0.00485975 0.00496828 0.00485975 0.00485975
  0.00485975 0.00485975 0.00512237 0.00485975 0.00485975 0.00497842
  0.00485975 0.00485975 0.00529747 0.00485975 0.00501855 0.00485975
  0.00485975 0.00485975 0.00517998 0.00485975 0.00485975 0.00486605
  0.00485975 0.00502372 0.00485975 0.00562595 0.00523695 0.00530743
  0.00485975 0.00524229 0.00503251 0.00526716 0.00485975 0.00513933
  0.00485975 0.00485975 0.00485975 0.00506664 0.00512645 0.00515922
  0.00485975 0.00540313 0.00485975 0.00485975 0.00485975 0.00518239
  0.00492453 0.00485975 0.00524862 0.00527168 0.00485975 0.00485975
  0.00505948 0.00485975 0.00485975 0.00485975 0.00523321 0.00505565
  0.00485975 0.00507832 0.00485975 0.00490269 0.00485975 0.00485975
  0.00485975 0.00485975 0.00485975 0.00485975 0.00505647 0.00485975
  0.00485975 0.00485975 0.0054276  0.00515477 0.00485975 0.00496317
  0.00485975 0.00506827 0.00541592 0.00529459 0.00485975 0.00547959
  0.00505889 0.00510788 0.00520929 0.00490939 0.005316   0.00487807
  0.00503018 0.00485975 0.00496746 0.00508338 0.00544687 0.00485975
  0.00485975 0.00485975 0.004934   0.00514647 0.00485975 0.00552855
  0.00485975 0.00485975 0.00485975 0.00504461 0.00485975 0.00486308
  0.0048958  0.00485975]]
Model(
  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(10, 25, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (conv4): Conv2d(25, 25, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=100, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=10, bias=True)
)
Output shape: torch.Size([2, 200])
Output logits:
[[0.00000000e+00 0.00000000e+00 0.00000000e+00 2.56512556e-02
  0.00000000e+00 2.73699430e-03 1.14887051e-01 0.00000000e+00
  2.84862109e-02 0.00000000e+00 0.00000000e+00 7.46243894e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.91777307e-02
  1.59131318e-01 0.00000000e+00 1.04753584e-01 7.43131936e-02
  0.00000000e+00 1.05268382e-01 0.00000000e+00 5.01745120e-02
  2.56845374e-02 7.52475411e-02 6.19827807e-02 8.65805745e-02
  0.00000000e+00 1.70032866e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 6.12703450e-02 6.11434355e-02 0.00000000e+00
  1.02086179e-01 0.00000000e+00 5.05326875e-02 2.71183066e-02
  4.52730106e-03 1.25422165e-01 0.00000000e+00 6.30268604e-02
  0.00000000e+00 0.00000000e+00 7.66082183e-02 7.01352060e-02
  2.68029124e-02 7.98833668e-02 0.00000000e+00 0.00000000e+00
  1.48028638e-02 1.36812150e-01 0.00000000e+00 1.38741788e-02
  4.30653505e-02 1.18973784e-01 2.79007219e-02 5.13556227e-02
  0.00000000e+00 1.01628937e-01 1.12491123e-01 0.00000000e+00
  3.81548293e-02 5.25853224e-02 0.00000000e+00 3.19562107e-02
  7.95850679e-02 2.60065813e-02 4.63629253e-02 0.00000000e+00
  1.18845455e-01 0.00000000e+00 0.00000000e+00 1.06589116e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.54450350e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  5.10502420e-02 0.00000000e+00 7.58666173e-02 9.67721492e-02
  1.43953469e-02 9.28983837e-02 3.94784398e-02 6.05982691e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.64644486e-02
  9.61279720e-02 0.00000000e+00 0.00000000e+00 5.43892197e-02
  4.07401696e-02 8.87620077e-02 1.12742618e-01 9.11391750e-02
  1.30186394e-01 1.01931706e-01 0.00000000e+00 3.74234356e-02
  0.00000000e+00 1.96471624e-02 0.00000000e+00 0.00000000e+00
  8.08472261e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  4.68739048e-02 0.00000000e+00 1.93782091e-01 0.00000000e+00
  0.00000000e+00 1.71185415e-02 5.60497530e-02 1.01477847e-01
  9.37972218e-02 0.00000000e+00 0.00000000e+00 2.82184705e-02
  7.76081607e-02 6.57490417e-02 4.52573374e-02 2.46831458e-02
  6.65991753e-02 5.25560156e-02 1.05599605e-01 6.45271465e-02
  4.14679684e-02 0.00000000e+00 1.03789084e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.01073556e-01 0.00000000e+00
  8.14977509e-04 1.37057811e-01 0.00000000e+00 0.00000000e+00
  1.90677438e-02 0.00000000e+00 0.00000000e+00 4.53362204e-02
  9.54795331e-02 5.22614941e-02 0.00000000e+00 5.69115244e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 3.38299461e-02 0.00000000e+00 3.54761095e-03
  0.00000000e+00 4.95014414e-02 0.00000000e+00 2.85610743e-03
  0.00000000e+00 0.00000000e+00 1.71642678e-04 6.98929951e-02
  0.00000000e+00 5.96804395e-02 3.85298170e-02 7.97927529e-02
  0.00000000e+00 2.28087939e-02 0.00000000e+00 1.36098802e-01
  0.00000000e+00 5.10003138e-03 0.00000000e+00 1.48680672e-01
  4.99012284e-02 1.19299561e-01 0.00000000e+00 0.00000000e+00
  8.58791471e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.53728379e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.28216118e-01 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.35824215e-02
  0.00000000e+00 0.00000000e+00 1.02738164e-01 0.00000000e+00
  3.79882976e-02 0.00000000e+00 0.00000000e+00 8.73120427e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.91490648e-02
  1.43127814e-01 0.00000000e+00 1.12901881e-01 5.99870011e-02
  8.28649150e-04 8.48951265e-02 0.00000000e+00 3.14330123e-02
  3.43477651e-02 5.30515574e-02 4.30734307e-02 9.71458331e-02
  0.00000000e+00 1.03986776e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 7.52464682e-02 4.18684222e-02 0.00000000e+00
  1.07039660e-01 0.00000000e+00 6.36303723e-02 1.35227684e-02
  0.00000000e+00 1.46831706e-01 0.00000000e+00 6.41485900e-02
  0.00000000e+00 0.00000000e+00 7.49427825e-02 5.47538586e-02
  3.61624956e-02 8.63060653e-02 0.00000000e+00 0.00000000e+00
  1.52907893e-02 1.38751194e-01 0.00000000e+00 6.62805000e-03
  4.26146463e-02 1.36442438e-01 4.13302556e-02 5.89431226e-02
  0.00000000e+00 1.07502326e-01 1.40948102e-01 0.00000000e+00
  5.63969687e-02 7.53939748e-02 0.00000000e+00 5.31140976e-02
  9.28327814e-02 1.72002800e-02 5.94743714e-02 0.00000000e+00
  1.39774010e-01 0.00000000e+00 0.00000000e+00 1.05107635e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.40689462e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  6.02115244e-02 0.00000000e+00 9.45217237e-02 9.36385319e-02
  0.00000000e+00 9.20310318e-02 2.62403805e-02 5.76458201e-02
  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.92922403e-02
  8.80537480e-02 0.00000000e+00 0.00000000e+00 3.70243378e-02
  3.85739356e-02 7.08647966e-02 1.04124382e-01 9.41750780e-02
  1.28169179e-01 1.05828509e-01 0.00000000e+00 5.33893891e-02
  0.00000000e+00 2.22147480e-02 0.00000000e+00 0.00000000e+00
  6.50478974e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
  3.55694890e-02 1.31676532e-02 2.06051439e-01 0.00000000e+00
  0.00000000e+00 4.65828786e-03 6.27159253e-02 1.03373803e-01
  9.04839784e-02 0.00000000e+00 0.00000000e+00 3.43321748e-02
  5.80515340e-02 4.52205874e-02 1.69087630e-02 4.11418453e-02
  5.71144186e-02 4.77775671e-02 1.27188101e-01 4.96753566e-02
  5.86683080e-02 0.00000000e+00 1.06632322e-01 0.00000000e+00
  0.00000000e+00 0.00000000e+00 1.04074173e-01 0.00000000e+00
  1.25628328e-02 1.24321945e-01 0.00000000e+00 0.00000000e+00
  3.95610631e-02 5.62095491e-04 0.00000000e+00 4.14017774e-02
  1.16322167e-01 3.02336328e-02 0.00000000e+00 7.63503686e-02
  2.20114272e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 1.58965774e-02 0.00000000e+00 0.00000000e+00
  0.00000000e+00 5.58699965e-02 1.77782420e-02 1.82830878e-02
  0.00000000e+00 0.00000000e+00 1.78569835e-02 6.19394481e-02
  0.00000000e+00 3.86553816e-02 2.99500506e-02 7.10406378e-02
  0.00000000e+00 6.64044265e-03 0.00000000e+00 1.54949382e-01
  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.35178119e-01
  4.47060540e-02 1.26116484e-01 0.00000000e+00 1.23733282e-02
  8.16135630e-02 0.00000000e+00 2.35127266e-02 0.00000000e+00
  0.00000000e+00 8.45103040e-02 0.00000000e+00 0.00000000e+00
  4.57655685e-03 0.00000000e+00 1.55293792e-01 0.00000000e+00]]
Output probabilities:
[[0.00482203 0.00482203 0.00482203 0.00494732 0.00482203 0.00483524
  0.00540909 0.00482203 0.00496137 0.00482203 0.00482203 0.00519564
  0.00482203 0.00482203 0.00482203 0.00501469 0.00565379 0.00482203
  0.00535456 0.00519402 0.00482203 0.00535732 0.00482203 0.00507014
  0.00494748 0.00519887 0.00513037 0.00525813 0.00482203 0.00490472
  0.00482203 0.00482203 0.00482203 0.00512671 0.00512606 0.00482203
  0.00534029 0.00482203 0.00507196 0.00495458 0.00484391 0.00546638
  0.00482203 0.00513573 0.00482203 0.00482203 0.00520595 0.00517236
  0.00495302 0.00522303 0.00482203 0.00482203 0.00489394 0.005529
  0.00482203 0.0048894  0.00503423 0.00543125 0.00495846 0.00507614
  0.00482203 0.00533785 0.00539615 0.00482203 0.00500957 0.00508238
  0.00482203 0.00497861 0.00522147 0.00494908 0.00505086 0.00482203
  0.00543055 0.00482203 0.00482203 0.0053644  0.00482203 0.00482203
  0.00482203 0.00489708 0.00482203 0.00482203 0.00482203 0.00482203
  0.00507459 0.00482203 0.00520209 0.00531199 0.00489195 0.00529145
  0.0050162  0.00512327 0.00482203 0.00482203 0.00482203 0.00510214
  0.00530857 0.00482203 0.00482203 0.00509156 0.00502254 0.00526961
  0.00539751 0.00528215 0.00549249 0.00533947 0.00482203 0.0050059
  0.00482203 0.0049177  0.00482203 0.00482203 0.00522807 0.00482203
  0.00482203 0.00482203 0.00505344 0.00482203 0.00585313 0.00482203
  0.00482203 0.00490529 0.00510002 0.00533705 0.00529621 0.00482203
  0.00482203 0.00496004 0.00521116 0.00514973 0.00504527 0.00494253
  0.00515411 0.00508223 0.00535909 0.00514344 0.00502619 0.00482203
  0.0053494  0.00482203 0.00482203 0.00482203 0.00533489 0.00482203
  0.00482596 0.00553036 0.00482203 0.00482203 0.00491486 0.00482203
  0.00482203 0.00504567 0.00530513 0.00508074 0.00482203 0.00510442
  0.00482203 0.00482203 0.00482203 0.00482203 0.00482203 0.00498795
  0.00482203 0.00483917 0.00482203 0.00506673 0.00482203 0.00483582
  0.00482203 0.00482203 0.00482286 0.00517111 0.00482203 0.00511857
  0.00501145 0.00522256 0.00482203 0.00493328 0.00482203 0.00552506
  0.00482203 0.00484668 0.00482203 0.00559501 0.00506876 0.00543302
  0.00482203 0.00482203 0.00525444 0.00482203 0.00482203 0.00482203
  0.00482203 0.00509657 0.00482203 0.00482203 0.00482203 0.00482203
  0.00548168 0.00482203]
 [0.00481934 0.00481934 0.00481934 0.00488524 0.00481934 0.00481934
  0.0053408  0.00481934 0.00500594 0.00481934 0.00481934 0.00525904
  0.00481934 0.00481934 0.00481934 0.00496189 0.00556093 0.00481934
  0.00539536 0.00511728 0.00482333 0.00524635 0.00481934 0.00497323
  0.00498775 0.00508192 0.00503146 0.00531101 0.00481934 0.00486972
  0.00481934 0.00481934 0.00481934 0.00519597 0.0050254  0.00481934
  0.00536382 0.00481934 0.00513596 0.00488495 0.00481934 0.00558156
  0.00481934 0.00513862 0.00481934 0.00481934 0.00519439 0.00509057
  0.00499681 0.00525375 0.00481934 0.00481934 0.0048936  0.00553664
  0.00481934 0.00485139 0.00502915 0.00552387 0.0050227  0.00511195
  0.00481934 0.0053663  0.00554882 0.00481934 0.00509895 0.00519674
  0.00481934 0.00508223 0.00528816 0.00490295 0.00511466 0.00481934
  0.00554231 0.00481934 0.00481934 0.00535347 0.00481934 0.00481934
  0.00481934 0.00488762 0.00481934 0.00481934 0.00481934 0.00481934
  0.00511843 0.00481934 0.0052971  0.00529242 0.00481934 0.00528392
  0.00494747 0.00510532 0.00481934 0.00481934 0.00481934 0.00506285
  0.00526294 0.00481934 0.00481934 0.00500112 0.00500887 0.00517325
  0.00534821 0.00529526 0.00547836 0.00535733 0.00481934 0.00508363
  0.00481934 0.0049276  0.00481934 0.00481934 0.00514325 0.00481934
  0.00481934 0.00481934 0.00499385 0.00488322 0.00592208 0.00481934
  0.00481934 0.00484184 0.00513127 0.00534419 0.00527575 0.00481934
  0.00481934 0.00498767 0.00510739 0.00504228 0.00490152 0.00502175
  0.00510261 0.00505518 0.00547299 0.00506479 0.00511054 0.00481934
  0.00536164 0.00481934 0.00481934 0.00481934 0.00534794 0.00481934
  0.00488027 0.00545733 0.00481934 0.00481934 0.00501382 0.00482205
  0.00481934 0.00502306 0.00541384 0.00496727 0.00481934 0.00520171
  0.00482996 0.00481934 0.00481934 0.00481934 0.00481934 0.00489656
  0.00481934 0.00481934 0.00481934 0.00509626 0.00490578 0.00490826
  0.00481934 0.00481934 0.00490617 0.00512729 0.00481934 0.00500928
  0.00496586 0.00517416 0.00481934 0.00485145 0.00481934 0.00562706
  0.00481934 0.00481934 0.00481934 0.00551689 0.00503968 0.00546713
  0.00481934 0.00487934 0.00522916 0.00481934 0.004934   0.00481934
  0.00481934 0.00524433 0.00481934 0.00481934 0.00484145 0.00481934
  0.00562899 0.00481934]]
